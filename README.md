# Reading

 - Deep Learning in NLP

## Deep Learning in NLP ##

- **BERT**: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". arXiv(2018)[[code][1]]
- **Survey on Attention**: "An Introductory Survey on Attention Mechanisms in NLP Problems". arXiv(2018)
- **Transformer**: "Attention is All you Need". NIPS(2017)[[code-tensorflow]](https://github.com/Kyubyong/transformer) [[code-pytorch]](https://github.com/jadore801120/attention-is-all-you-need-pytorch)
- **Memory Networks**: "End-To-End Memory Networks". NIPS(2015)
- **Pointer Networks**: "Pointer Networks". NIPS(2015)
- **Copying Mechanism**: "Incorporating Copying Mechanism in Sequence-to-Sequence Learning". ACL(2016)
- **Multi-task Learning**: "An Overview of Multi-Task Learning in Deep Neural Networks". arXiv(2017)
- **Multi-domain multi-task**: "A Unified Perspective on Multi-Domain and Multi-Task Learning". ICLR(2015)


  [1]: https://github.com/google-research/bert
